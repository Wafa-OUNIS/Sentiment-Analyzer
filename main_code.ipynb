{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <center> <font color='#8A0829'> Sentiment analyser for OOREDOO Tunisia</font> </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='#B43104'> Context of the project</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This project consists of an implementation of a sentiment analyzer for the phone operator OOREDOO Tunisia using social media comments written in Tunisian Arabic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'> Import necessary libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import entire libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#so that plots appear in the same browser window\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'> Data Preparation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data we used is a collection of 341 comments, collected from Ooredoo Facebook official page and Tweets that contain #ooredootn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading data from the csv file\n",
    "df0 = pd.read_csv('ooredoodata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in df0['message']:\n",
    "     i=i.replace('?', '')\n",
    "     i=i.replace('!', '')\n",
    "     i= i.lower()\n",
    "     df0['message'].replace(i)\n",
    "     #print(i) \n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have 2 sentiment categories: positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(df0['category']).most_common()\n",
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this graph shows the distribution of the positive and negative comments in the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 2 artists>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nztBLUuf+DxcnhqdInjisAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x490bf54400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gR = df0.groupby('category').size()\n",
    "\n",
    "plt.bar(gR.index, gR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'> Split data into train and test sets </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df0['message'], \n",
    "                                                    df0['category'], \n",
    "                                                    random_state=591)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255,)\n",
      "(86,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will build a sentiment analayser using  different document representation and classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'>1. Sentiment analysis based on BOW model with word occurrences</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'>Feature extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Feature extraction* means representing raw text documents as numerical *feature vectors*.\n",
    "- In the simple BOW model, feature vector = number of word occurrences for each document and each vocabulary word.\n",
    "- We will do this using the ``CountVectorizer`` class: first we'll **tokenize** the documents and extract the vocabulary set, and then we determine the feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'> Tokenize documents and build vocabulary set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit the CountVectorizer to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#B43104'> Construction of document-term matrix </font>\n",
    "- This matrix contains the *feature vectors* of a given set of raw documents.\n",
    "- For the simple BOW model, feature vector = number of word occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 1289)\n",
      "(86, 1289)\n"
     ]
    }
   ],
   "source": [
    "# the document-term matrix for the training corpus\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "\n",
    "# the document-term matrix for the test corpus\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "print(X_test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'> Build classification model using Logistic Regression </font>\n",
    "We are going to  to build a classification model using the feature vectors of the training documents (which are stored in the variable ``X_train_vectorized``) and their corresponding true sentiment categories (which are stored in the variable ``y_train``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using LR method\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'> Test the classification model </font>\n",
    "We'll use the obtained LR model to predict sentiment categories (classes) of test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "LR_predictions = LR_model.predict(X_test_vectorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "those some exemples of the predictions for the first 10 test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive' 'negative' 'positive' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'negative']\n",
      "[ 'Chère Ooredoo sabit jom3a hethi 3 cartes w mjani 7ata bonus donc demain nchouf a9reb boutique sinon na7i ligne car je suis en colère'\n",
      " 'in7eb ooredoo Wa ana zaboun wafi lah Wa piisati klha ooredoo walhi n7eki bl7ek wanchlh narb7'\n",
      " 'Ouuh amsat pub ritha f 7yatii ????'\n",
      " 'Ooredoo Tunisie  Community manager mta3kom berjoulia yestahil Bonus fil chahria wel equipe kamla !'\n",
      " 'Berabi a3touni na hale rani ne9ouse meta3 chehar fi neharine te9ouse meta3 jeme3 nehar a3touni fekera berabi'\n",
      " 'oouh 3al connexion' 'Ena vraiment publicité ma3jbnich ????'\n",
      " \"Service techniques  zero  3andi presque 2 mois w Ena na3mel f des  réclamations  concernant  ligne mta3i matab3athech  des messages w chay service zerooooo jusqu'à  maintenant  mafama  7ata résultat!!\"\n",
      " 'kol 3am w ooredoo fl 3alali '\n",
      " '125 Mo ! Zaaaa7 rana fi 2017 rahou mch 1997.... Tadh7kou 3al 3bed... A3mlou 1 Go b 0.900 taw ness lkol tefra7... 9allou 125 Mo']\n"
     ]
    }
   ],
   "source": [
    "print(LR_predictions[:10])\n",
    "print(np.array(X_test[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#B43104'> Evaluate performance of classification model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77906976744186052"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_classif_rate = accuracy_score(y_test, LR_predictions)\n",
    "LR_classif_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#B43104'>  Interpretation of model's coefficients (parameters)</font>\n",
    "Which vocabulary words are most important in our classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs of LR model:\n",
      "\n",
      " Coefficient values:\n",
      "[-0.76013548 -0.67214242 -0.67180336 -0.65640947 -0.65303264 -0.6512088\n",
      " -0.64966107 -0.59014885 -0.58154002 -0.54518596]\n",
      "\n",
      " Feature names:\n",
      "['sor3a' 'yabdou' 'ma3ndich' 'breseau' '3enk' 'kifech' 'wamiya' '4achach'\n",
      " 'fi' 'man7ebch']\n",
      "\n",
      "Largest Coefs of LR model:\n",
      "\n",
      " Coefficient values: \n",
      "[ 1.46563069  0.91607536  0.80236102  0.771239    0.72802291  0.72802291\n",
      "  0.72032129  0.69000152  0.67240607  0.65082873]\n",
      " Feature names: \n",
      "['oredoo' 'bil' 'mafara7to' 'bugs' 'oridoo' 'modem' 'page' 'chouti'\n",
      " 'partagée' 'fidele']\n",
      "Smallest abs(Coefs):\n",
      "\n",
      " Coefficient values:\n",
      "[ -5.37551997e-07  -6.34311357e-05  -3.42090049e-03  -7.06375012e-03\n",
      "  -7.34932721e-03  -8.26432590e-03  -8.26432590e-03  -8.26432590e-03\n",
      "  -8.26432590e-03  -8.26432590e-03]\n",
      "\n",
      " Feature names:\n",
      "['tlf' 'alch' 'aman' 'orange' 'kemel' 'athaaaaaaammlmkkkllll' 'atwo'\n",
      " 'ezhar' 'ra8m' 'ba3id']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the 10 smallest and 10 largest coefficients\n",
    "\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "print('Smallest Coefs of LR model:\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs[sorted_coef_index[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "\n",
    "print('Largest Coefs of LR model:\\n')\n",
    "print(' Coefficient values: \\n{}'.format(coefs[sorted_coef_index[:-11:-1]]))\n",
    "print(' Feature names: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n",
    "\n",
    "print('Smallest abs(Coefs):\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs[sorted_coef_index_2[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names[sorted_coef_index_2[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'> Build classification model using Naive Bayes</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Naive Bayes classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " predict sentiment of test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_predictions = NB_model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate model's classification rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76744186046511631"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_classif_rate = accuracy_score(y_test, NB_predictions)\n",
    "NB_classif_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#B43104'> 2.  Sentiment analysis using BOW model and Tfidf</font>\n",
    "We are basically going to re-do the same steps as above, but using the ``TfidfVectorizer`` class instead of ``CountVectorizer`` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we Build vocabulary and then we fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(min_df=5)\n",
    "tfidf_vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we Build document-term matrices: X_train_vectorized_2 and X_test_vectorized_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_vectorized_2 = tfidf_vect.transform(X_train)\n",
    "X_test_vectorized_2 = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Now we build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_model_2 = LogisticRegression()\n",
    "LR_model_2.fit(X_train_vectorized_2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we calculate the classification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79069767441860461"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate the classifier, i.e. calculate its classification rate\n",
    "\n",
    "LR_predictions_2 = LR_model_2.predict(X_test_vectorized_2)\n",
    "accuracy_score(y_test, LR_predictions_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we search for the most significant vocabulary words according to their max tfidf feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary words sorted by tfidf feature value:\n",
      "  Smallest:\n",
      "['ou' 'année' 'ti' 'dinar' 'bonus' 'n7eb' 'flous' 'solde' 'ken' 'ena']\n",
      "\n",
      "  Largest: \n",
      "['ya' 'el' 'les' 'la' 'kol' 'internet' 'fil' 'fi' 'dima' 'mala']\n"
     ]
    }
   ],
   "source": [
    "sorted_tfidf_index = X_train_vectorized_2.max(0).toarray()[0].argsort()\n",
    "print('Vocabulary words sorted by tfidf feature value:')\n",
    "print('  Smallest:\\n{}\\n'.format(tfidf_feature_names[sorted_tfidf_index[:10]]))\n",
    "print('  Largest: \\n{}'.format(tfidf_feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we search for the most significant vocabulary words according to their LR model coefficient value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary words sorted by LR model coefficient value:\n",
      "  Smallest Coefs:\n",
      "['sor3a' 'yabdou' 'ma3ndich' 'breseau' '3enk' 'kifech' 'wamiya' '4achach'\n",
      " 'fi' 'man7ebch']\n",
      "\n",
      "  Largest Coefs: \n",
      "['oredoo' 'bil' 'mafara7to' 'bugs' 'oridoo' 'modem' 'page' 'chouti'\n",
      " 'partagée' 'fidele']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef2_index = LR_model_2.coef_[0].argsort()\n",
    "print('Vocabulary words sorted by LR model coefficient value:')\n",
    "print('  Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('  Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
